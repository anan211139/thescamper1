{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "def padding_square_resize_evi(im):\n",
    "  img = im.transpose([1,2,0])\n",
    "  old_image_height, old_image_width, channels = img.shape\n",
    "  new_image_width = max(img.shape)\n",
    "  new_image_height = max(img.shape)\n",
    "  color = (0)\n",
    "  result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "  x_center = (new_image_width - old_image_width) // 2\n",
    "  y_center = (new_image_height - old_image_height) // 2\n",
    "  \n",
    "  result[y_center:y_center+old_image_height, \n",
    "          x_center:x_center+old_image_width] = img\n",
    "  return cv2.resize(result,(32,32))\n",
    "def onehot(label):\n",
    "  a = np.zeros(4)\n",
    "  a[int(label)-1]=1\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/labels.csv', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import cv2\n",
    "nocloud = '/content/drive/MyDrive/arv/Nocloud/'\n",
    "data_all= []\n",
    "labels_all = []\n",
    "labels = []\n",
    "for foldername in sorted(os.listdir(nocloud)):\n",
    "  bound = []\n",
    "  for filename in sorted([os.path.basename(a) for a in glob.glob(nocloud + foldername + '/*.tif')]):\n",
    "    with rasterio.open(nocloud + foldername + '/' + filename) as src:\n",
    "        band = src.read()\n",
    "    bound.append(padding_square_resize_evi(band))\n",
    "  data_all.append(np.array(bound))\n",
    "  labels_all.append(onehot(df.iloc[int(filename)-1][1]))\n",
    "  labels.append(df.iloc[int(filename)-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = np.array(data_all)\n",
    "labels_all = np.array(labels_all)\n",
    "print(np.array(data_all).shape,np.array(labels_all).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = np.transpose(data_all, [0,2,3,1])\n",
    "da =da.reshape(1317,32,38*32,-1)\n",
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(da , labels_all, test_size=0.2 , shuffle = True)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import MaxPool2D, Conv2D, Dropout, Dense,Flatten\n",
    "\n",
    "inputs = layers.Input(shape=(x_train.shape[1] , x_train.shape[2], x_train.shape[3]))\n",
    "x = Conv2D(64, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu')(inputs)\n",
    "x = Conv2D(32, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(4, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, x)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(class_weight = \"balanced\" , classes = np.unique(labels) , y = np.array(labels))\n",
    "class_weights = {i:w for i,w in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "modelPath = '/content/model{epoch:02d}.hdf5'\n",
    "earlystopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.001, \n",
    "    patience=8, \n",
    "    restore_best_weights=True, \n",
    "    )\n",
    "callbacksList = [earlystopping] #, callbacks=callbacksList\n",
    "model_history = model.fit( x_train , y_train, epochs=100,  callbacks=callbacksList, batch_size = 64, validation_data = ( x_test, y_test ) , class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
